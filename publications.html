<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Publications | OpenDataLab</title>
  <link rel="stylesheet" href="css/style.css">
  <script src="js/main.js" defer></script>
</head>
<body>
  <header>
    <div class="container nav-container">
      <nav>
        <a href="index.html">Home</a>
        <a href="research.html">Research</a>
        <a href="people.html">People</a>
        <a href="publications.html" class="active">Publications</a>
        <a href="news.html">News</a>
        <a href="resources.html">Resources</a>
        <a href="opportunity.html">Opportunities</a>
      </nav>
    </div>
  </header>  
  <main>
    <!-- content-wrapper 将由JavaScript添加包裹 -->
    <div class="container">
      <h2>Selected Publications</h2>

      <section class="publication-category">
        <h3>Data Intelligence</h3>
        <ul>
          <li>Junyan Ye*, Baichuan Zhou*, Zilong Huang*, Junan Zhang*, Tianyi Bai*, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, Zhizheng Wu, Yiping Chen, Dahua Lin, Conghui He<sup>†</sup>, Weijia Li<sup>†</sup>. LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models <strong>ICLR 2025</strong> <strong>Spotlight</strong></li>
          <li>Chi Zhang*, Huaping Zhong*, Kuan Zhang, Chengliang Chai<sup>†</sup>, Rui Wang, Xinlin Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ju Fan, Ye Yuan, Guoren Wang, Conghui He<sup>†</sup>. Harnessing Diversity for Important Data Selection in Pretraining Large Language Models <strong>ICLR 2025</strong> <strong>Spotlight</strong></li>
          <li>Linke Ouyang*, Yuan Qu*, Hongbin Zhou*, Jiawei Zhu*, Rui Zhang*, Qunshu Lin*, Bin Wang*, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, Conghui He<sup>†</sup>. OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations <strong>CVPR 2025</strong></li>
          <li>Bin Wang*, Fan Wu*, Linke Ouyang*, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, Conghui He<sup>†</sup>. Image Over Text: Transforming Formula Recognition Evaluation with Character Detection Matching <strong>CVPR 2025</strong></li>
          <li>Qingyun Li*, Zhe Chen*, Weiyun Wang*, Wenhai Wang*, Shenglong Ye*, Zhenjiang Jin*, Guanzhou Chen*, Yinan He*, Zhangwei Gao*, Erfei Cui*, Jiashuo Yu*, Hao Tian*, Jiasheng Zhou*, Chao Xu*, Bin Wang*, Xingjian Wei*, Wei Li*, Wenjian Zhang*, Bo Zhang*, Pinlong Cai*, Licheng Wen*, Xiangchao Yan*, Zhenxiang Li*, Pei Chu*, Yi Wang*, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Zhongying Tu*, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He<sup>†</sup>, Jifeng Dai<sup>†</sup>. OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text <strong>ICLR 2025</strong></li>
          <li>Xinlin Zhuang*, Jiahui Peng*, Ren Ma*, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, Conghui He<sup>†</sup>. Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models <strong>Arxiv 2025</strong></li>
          <li>Siwei Wen*, Junyan Ye*, Peilin Feng, Hengrui Kang, Zichen Wen, Yize Chen, Jiang Wu, Wenjun Wu, Conghui He, Weijia Li<sup>†</sup>. Spot the fake: Large multimodal model-based synthetic image detection with artifact explanation <strong>Arxiv 2025</strong></li>
          <li>Hengrui Kang*, Siwei Wen*, Zichen Wen*, Junyan Ye, Weijia Li<sup>†</sup>, Peilin Feng, Baichuan Zhou, Bin Wang, Dahua Lin, Linfeng Zhang, Conghui He<sup>†</sup>. LEGION: Learning to Ground and Explain for Synthetic Image Detection <strong>Arxiv 2025</strong></li>
          <li>Tianyi Bai, Ling Yang, Zhen Hao Wong, Jiahui Peng, Xinlin Zhuang, Chi Zhang, Lijun Wu, Jiantao Qiu<sup>†</sup>, Wentao Zhang<sup>†</sup>, Binhang Yuan, Conghui He<sup>†</sup>. Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining <strong>Arxiv 2024</strong></li>
          <li>Zhiyuan Zhao*, Hengrui Kang*, Bin Wang, Conghui He<sup>†</sup>. DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception <strong>Arxiv 2024</strong></li>
        </ul>
      </section>

      <section class="publication-category">
        <h3>Large Language Models and Multimodal LLMs</h3>
        <ul>
          <li>Jiaxing Sun*, Weiquan Huang*, Jiang Wu*, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He<sup>†</sup>. Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations <strong>ACL 2024</strong></li>
          <li>Yiqi Lin*, Conghui He*<sup>†</sup>, Alex Jinpeng Wang*, Bin Wang*, Weijia Li, Mike Zheng Shou. Parrot Captions Teach CLIP to Spot Text <strong>ECCV 2024</strong></li>
          <li>Bin Wang*, Fan Wu*, Xiao Han*, Jiahui Peng*, Huaping Zhong*, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, Conghui He<sup>†</sup>. VIGC: Visual Instruction Generation and Correction <strong>AAAI 2024</strong></li>
          <li>Runchuan Zhu*, Zhipeng Ma*, Jiang Wu*, Junyuan Gao, Jiaqi Wang, Dahua Lin, Conghui He<sup>†</sup>. Utilize the Flow Before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal-Aware Instruction Tuning <strong>AAAI 2025</strong></li>
          <li>Baichuan Zhou*, Haote Yang*, Dairong Chen*, Junyan Ye*, Tianyi Bai, Jinhua Yu, Songyang Zhang, Dahua Lin, Conghui He<sup>†</sup>, Weijia Li<sup>†</sup>. Urbench: A comprehensive benchmark for evaluating large multimodal models in multi-view urban scenarios <strong>AAAI 2025</strong></li>
          <li>Runchuan Zhu*, Zinco Jiang*, Jiang Wu*, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu, Conghui He<sup>†</sup>. GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation <strong>NAACL 2025</strong> <strong>Findings</strong></li>
        </ul>
      </section>

      <section class="publication-category">
        <h3>AI for Science</h3>
        <ul>
          <li>Qizhi Pei, Rui Yan<sup>†</sup>, Kaiyuan Gao, Jinhua Zhu, Lijun Wu<sup>†</sup>. 3D-MolT5: Towards Unified 3D Molecule-Text Modeling with 3D Molecular Tokenization <strong>ICLR 2025</strong></li>
          <li>Zizhuo Zhang, Lijun Wu<sup>†</sup>, Kaiyuan Gao, Jiangchao Yao, Tao Qin, Bo Han<sup>†</sup>. Fast and Accurate Blind Flexible Docking <strong>ICLR 2025</strong></li>
          <li>Chao Pang*, Xingxing Weng*, Jiang Wu*, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia<sup>†</sup>, Conghui He<sup>†</sup>. VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis <strong>AAAI 2025</strong></li>
          <li>Junyan Ye, Qiyan Luo, Jinhua Yu, Huaping Zhong, Zhimeng Zheng, Conghui He, Weijia Li<sup>†</sup>. SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation <strong>CVPR 2024</strong> <strong>Highlight</strong></li>
          <li>Weijia Li*, Haote Yang*, Zhenghao Hu, Juepeng Zheng, Gui-Song Xia, Conghui He<sup>†</sup>. 3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions <strong>CVPR 2024</strong></li>
          <li>Junyan Ye, Zhutao Lv, Weijia Li<sup>†</sup>, Jinhua Yu, Haote Yang, Huaping Zhong, Conghui He<sup>†</sup>. Cross-view image geo-localization with Panorama-BEV Co-Retrieval Network <strong>ECCV 2024</strong></li>
          <li>Weijia Li, Yawen Lai, Linning Xu, Yuanbo Xiangli, Jinhua Yu, Conghui He<sup>†</sup>, Gui-Song Xia<sup>†</sup>, Dahua Lin. Omnicity: Omnipotent city understanding with multi-level and multi-view images <strong>CVPR 2023</strong></li>
        </ul>
      </section>
    </div>
  </main>
  
  <footer>
    <div class="container">
      <div class="footer-content">
        <div class="footer-contact">
          <p>Shanghai Artificial Intelligence Laboratory · OpenDataLab | Email: xxxxxxx</p>
        </div>
        <div class="footer-copyright">
          <p>Copyright © 2025 OpenDataLab | All Rights Reserved</p>
        </div>
      </div>
    </div>
  </footer>
</body>
</html> 